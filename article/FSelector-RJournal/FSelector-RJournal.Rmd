---
title: Rcpp Implementation of Entropy Based Feature Selection Algorithms with Sparse Matrix Support
author:
  - name: Zygmunt Zawadzki
    email:  zygmunt@zstat.pl
  - name: Marcin KosiÅ„ski
    affiliation: Warsaw Univeristy of Technology
    address:
    - Faculty of Mathematics and Information Science
    - Koszykowa 75, Warsaw Poland
    email:  m.kosinski@mini.pw.edu.pl
abstract: > 
  Feature selection is a process of extracting valuable features that have significant influence on dependent variable. Time efficient feature selection algorithms are still an active field of research and are in the high demand in the machine learning area.  
  \par We introduce \pkg{FSelectorRcpp}, an \R \ package \citep{R} that includes entropy based feature selection algorithms. Methods presented in this package are not new, they were reimplemented in C++ and originally come from \pkg{FSelector} package \citep{FSelector}, but we provide many technical improvements. Our reimplementation occures to have shorter computation times, it does not require earlier Java nor Weka \citep{Hall:2009:WDM:1656274.1656278} installation and provides support for sparse matrix format of data, e.g. presented in \pkg{Matrix} package \citep{Matrix}. This approach facilitates software installation and improves work with bigger datasets, in comparison to the base \R \ implementation in \pkg{FSelector}, which is even not optimal in the sense of \R \ code.
  \par Additionally, we present new, C++ implementation of the discretization method for continuous variables called Multi-Interval Discretization (MDL) method \citep{Fayyad1993} that is based on *Minimum Description Length* critetion \citep{rissanen1986} and is required in entropy calculations during the feature selection process in showed methods. By default, regular \pkg{FSelector} implementation uses \pkg{RWeka} package \citep{Hornik2009} for discretization and \pkg{entropy} \citep{entropy} for entropy calculations - for both we also attach the computation times comparison.
  \par Finally, we announce the full list of available functions, which are divided to 2 groups: entropy based feature selection methods and stepwise attribute selection functions that might use any evaluator to choose propoer features, e.g. presented entropy based algorithms.
preamble: >
  % Any extra latex you need in the preamble
output: rticles::rjournal_article
---

```{r include = FALSE}
library(knitr)
opts_chunk$set(
	comment = "",
	fig.width = 12, 
	message = FALSE,
	warning = FALSE,
	tidy.opts = list(
		keep.blank.line = TRUE,
		width.cutoff = 100
		),
	options(width = 100),
	eval = TRUE
)
```


# Introduction and Motivation

In the modern statistical learning the biggest bottlenecks are computation times of model training procedures and the overfitting. Both are caused by the same issue - the high dimension of explanatory variables space. Researchers have encountered problems with too big sets of features used in machine learning algorithms also in terms of model interpretaion. This motivates applying feature selection algorithms before performing statisical modeling, so that on smaller set of attributes the training time will be shorter, the interpretation might be clearer and the noise from non important features can be avoided. More motivation can be found in \cite{John94irrelevantfeatures}.
\par Many methods were developed to reduce the curse of dimensionality like Principal Component Analysis \citep{PCA:14786440109462720} or Singular Value Decomposition \citep{eckart1936approximation} which approximates the variables by smaller number of combinations of original variables, but this approach is hard to interpret in the final model. 
\par Sophisticated methods of attribute selection as Boruta algoritm \citep{Boruta}, genetic algorithms \citep{geneticAlgo, FedCSIS2013l106} or simulated annealing techniques \citep{Khachaturyan:a19748} are known and broadly used but in some cases for those algorithms computations can take even days, not to mention that datasets are growing every hour.
\par Few classification and regression models can reduce redundand variables during the training phase of statistical learning process, e.g. Decision Trees \citep{Rokach:2008:DMD:1796114, cart84}, LASSO Regularized Generalized Linear Models (with cross-validation) \citep{glmnet} or Regularized Support Vector Machine \citep{Xu:2009:RRS:1577069.1755834}, but still computations starting with full set of explanatory variables are time consuming and the understaning of the feature selection procedure in this case is not simple and those methods are sometimes used without the understanding.
\par In business applications there appear a need to provide a fast feature selection that is extremely easy to understand. For such demands easy methods are prefered. This motivates using simple techniques like Entropy Based Feature Selection \citep{Largeron:2011:EBF:1982185.1982389}, where every feature can be checked independently so that computations can be performed in a parallel to shorter the procedure's time. For this approach we provide an \R \ interface to Rcpp reimplementation \citep{Rcpp} of methods included in \pkg{FSelector} package which we also extended with parallel background and sparse matrix support. This has significant impact on computations time and can be used on greater datasets, comparing to \pkg{FSelector}. Additionally we avoided the Weka \citep{Hall:2009:WDM:1656274.1656278} dependency and we provided faster discretization implementations than those from \pkg{entropy} package, used originally in \pkg{FSelector}.

# Discretization

In statistical modelling, the **discretization** is the process of transferring continuous explanatory variables into discrete counterparts. Problems caused by categorization of continuous variables are known and widely spread \citep{HarrellDISC}, but in some cases there appear an algorithmic requirement for the discretization. Moreover, there exist few algorithms, like decision trees \citep{Salzberg1994}, where continuous attributes are discretized during the learning process.  Other reason for variable discretization include increasing the speed of induction algorithms \citep{Catlett1991}. 
\par Even though many categorization algorithms have been developed \citep{Holte1993, Chan169942}, in this chapter we focus on a recursive entropy minimization heuristic for categorization coupled with a *Minimum Description Length* critetion \citep{rissanen1986} that controls the number of intervals produced over the continuous space. This is motivated by the original usage of this algorithm in \pkg{FSelector} package, which is our baseline. As \cite{Dougherty95supervisedand} showed better performance of classification for discretized feature set on real-world datasets and states that the described method was found promising by the authors not only for the local discretization but also for global discretization \citep{Ting94discretizationof}.

## Entropy Based Minimum Description Length Discretization Method

In this section we pressent the overview of the method, but before that let us introduce some terminology, after \citep{Fayyad1993}. 

\par Assume we are to discretize continuous variable $A$, having set $S$ of $N$ observations. As a *cut point*, we call such a treshold value $T$ that $A<=T$ assigns observations to left interval of S and $A>T$ assigns observations to right interval of S\footnotetext{The $A > T$ means: the values of $A$ are greater than $T$}. For the ordered $A$ the algorithm is looking for the best cut point $T_A$ from the range of $A$ by evaluating *every candidate cut point* in the range of values. For each evaluation of a candidate cut point $T$, the data are partitioned into two sets and the class entropy of resulting partition is calucalted.

\par Let *T* partition the set $S$ into subsets $S_1$ and $S_2$. For $k$ classes ($C_1,\dots,C_k$) let $\mathbb{P}(C_i, S)$ be proportion of observations that belongs to class $C_i$. The *class entropy* of a subset S is defined as: $$Ent(S) = - \sum_{i=1}^{k}\mathbb{P}(C_i, S)\log_{2}\mathbb{P}(C_i, S).$$

This is said to to measure the information needed (in *bits*) to specify the classes in $S$. Having this definition one can specify, for partitioned set $S$, the weighted average of the resulting class entropies:

\textbf{Definition 1.} For an example set $S$, an attribute $A$ and a cut value $T$; let $S_1 \subset S$ be the subset of observations in $S$ with $A$ values $<= T$ and $S_2 = S - S_1$. The *class information entropy of the partition induced by T, E(A, T; S)*, is defined 
\begin{equation}
E(A, T; S) = \frac{|S_1|}{S}Ent(S_1) + \frac{|S_2|}{S}Ent(S_2).
\end{equation}

A binary discretization for $A$ is determined by selecting the cut point $T_A$ for which $E(A, T; S)$ is minimal amongst all the candidate cut points. 

\par Choosing beetwen all possible values of $A$, when looking for optimal $T_A$, could be very time and computations consuming, but in \cite{Fayyad:1992:IDT:144532} it is proved that if $T$ is an optimal cut point (minimizes the E(A, T; S)) then $T$ is a *boundary point*.

\textbf{Definition 2.} A value $T$ in the range of $A$ is a *boundary point* if in the sequence of examples sorted by the value of $A$, there exist two points $\epsilon_1, \epsilon_2 \in S$, having different classes, such that $A(\epsilon_1) < T < A(\epsilon_2)$; such that there exists no other example $\epsilon^1 \in S$ such that $A(\epsilon_1) < A(\epsilon^1) < A(\epsilon_2)$, where $A(\epsilon)$ denote the $A$ value of example $\epsilon \in S$.


- Backward compatibility with \pkg{FSelector}

```{r}
library(RWeka)
library(FSelectorRcpp)
RWeka::Discretize(Species~Sepal.Length, data = iris)[, 1] -> Rweka_disc_out
FSelectorRcpp::discretize(iris$Sepal.Length, iris$Species) ->FSelectorRcpp_disc_out
table(Rweka_disc_out,FSelectorRcpp_disc_out)
```

- Time comparison on small datasets

```{r}
library(microbenchmark)
microbenchmark(discretize(iris$Sepal.Length, iris$Species),
               Discretize(Species~Sepal.Length, data = iris))
```

- Comparison on big datasets

- Plot comparison


# Entropy Based Feature Selection Algorithms

In the information theory the term **entropy** \citep{Shannon:2001:MTC:584091.584093} is 

# Stepwise Attribute Selection Evaluators

# FSelectorRcpp and FSelector Computation Times Comparison

# Conclusion

# Acknowledgment


\bibliography{RJreferences}
