% !TeX root = RJwrapper.tex
\title{Rcpp Implementation of Entropy Based Feature Selection Algorithms with
Sparse Matrix Support}
\author{by Zygmunt Zawadzki, Marcin Kosiński}

\maketitle

\abstract{%
Feature selection is a process of extracting valuable features that have
significant influence on dependent variable. Time efficient feature
selection algorithms are still an active field of research and are in
the high demand in the machine learning area. \par We introduce
\pkg{FSelectorRcpp}, an \R ~package \citep{R} that includes entropy
based feature selection algorithms. Methods presented in this package
are not new, they were reimplemented in C++ and originally come from
\pkg{FSelector} package \citep{FSelector}, but we provide many technical
improvements. Our reimplementation occures to have shorter computation
times, it does not require earlier Java nor Weka
\citep{Hall:2009:WDM:1656274.1656278} installation and provides support
for sparse matrix format of data, e.g.~presented in \pkg{Matrix} package
\citep{Matrix}. This approach facilitates software installation and
improves work with bigger datasets, in comparison to the base
\R ~implementation in \pkg{FSelector}, which is even not optimal in the
sense of \R ~code. \par Additionally, we present new, C++ implementation
of continuous variables Multi-Interval Discretization (MDL) method
\citep{Fayyad1993}, which is required in entropy calculations during the
feature selection process in showed methods. By default, regular
\pkg{FSelector} implementation uses \pkg{entropy} package
\citep{entropy}, for which we also attach the computation times
comparison. \par Finally, we announce the full list of available
functions, which are divided to 2 groups: entropy based feature
selection methods and stepwise attribute selection functions that might
use any evaluator to choose propoer features, e.g.~presented entropy
based algorithms.
}

\section{Introduction and Motivation}\label{introduction-and-motivation}

In modern statistical learning the biggest bottlenecks are computation
times of model training procedures and the overfitting. Both are caused
by the same issue - the high dimension of explanatory variables space.
Researchers have encountered problems with too big sets of features used
in machine learning algorithms also in terms of model interpretaion.
This motivates applying feature selection algorithms before performing
statisical modeling, so that on smaller set of attributes the training
time will be shorter, the interpretation might be clearer and the noise
from non important features can be avoided. More motivation can be found
in \cite{John94irrelevantfeatures}. \par Many methods were developed to
reduce the curse of dimensionality like Principal Component Analysis
\citep{PCA:14786440109462720} or Singular Value Decomposition
\citep{eckart1936approximation} which approximates the variables buy
smaller number of combinations of original variables, but this approach
is hard to interpret in the final model. \par Sophisticated methods of
attribute selection as Boruta algoritm \citep{Boruta}, genetic
algorithms \citep{geneticAlgo, FedCSIS2013l106} or simulated annealing
techniques \citep{Khachaturyan:a19748} are known and broadly used but in
some cases for those algorithms computations can take even days, not to
mention that datasets are growing every day. \par Few classification and
regression models can reduce redundand variables during the training
phase of statistical learning process, e.g.~Decision Trees
\citep{Rokach:2008:DMD:1796114, cart84}, LASSO Regularized Generalized
Linear Models (with cross-validation) \citep{glmnet} or Regularized
Support Vector Machine \citep{Xu:2009:RRS:1577069.1755834}, but still
computations starting with full set of explanatory variables are time
consuming and the understaning of the feature selection procedure in
this case is not simple and those methods are sometimes used without the
understanding. \par In business applications there appear a need to
provide a fast feature selection that is extremely easy to understand.
For such demands easy methods are prefered. This motivates using simple
techniques like Entropy Based Feature Selection
\citep{Largeron:2011:EBF:1982185.1982389}, where every feature can be
checked independently so that computations can be performed in a
parallel to shorter the procedure's time. For this approach we provide
an \R ~interface to Rcpp reimplementation of methods included in
\pkg{FSelector} package which we also extended with parallel background
and sparse matrix support. This has significant impact on computations
time and can be used on greater datasets than regular \R ~implementation
prepared by \pkg{FSelector} original author, Piotr Romański.

\section{Discretization}\label{discretization}

\section{Entropy Based Feature Selection
Algorithms}\label{entropy-based-feature-selection-algorithms}

In the information theory the term \textbf{entropy}
\citep{Shannon:2001:MTC:584091.584093} is

\section{Stepwise Attribute Selection
Evaluators}\label{stepwise-attribute-selection-evaluators}

\section{FSelectorRcpp and FSelector Computation Times
Comparison}\label{fselectorrcpp-and-fselector-computation-times-comparison}

\section{Conclusion}\label{conclusion}

\section{Acknowledgment}\label{acknowledgment}

\bibliography{RJreferences}

\address{%
Zygmunt Zawadzki\\
\\
\\
}
\href{mailto:zygmunt@zstat.pl}{\nolinkurl{zygmunt@zstat.pl}}

\address{%
Marcin Kosiński\\
Warsaw Univeristy of Technology\\
Faculty of Mathematics and Information Science\\ Koszykowa 75, Warsaw Poland\\
}
\href{mailto:m.kosinski@mini.pw.edu.pl}{\nolinkurl{m.kosinski@mini.pw.edu.pl}}

