% !TeX root = RJwrapper.tex
\title{Rcpp Implementation of Entropy Based Feature Selection Algorithms with
Sparse Matrix Support}
\author{by Zygmunt Zawadzki, Marcin Kosi≈Ñski}

\maketitle

\abstract{%
Feature selection is a process of extracting valuable features that have
significant influence on dependent variable. Time efficient feature
selection algorithms are still an active field of research and are in
the high demand in the machine learning area. \par We introduce
\pkg{FSelectorRcpp}, an \R ~package \citep{R} that includes entropy
based feature selection algorithms. Methods presented in this package
are not new, they were reimplemented in C++ and originally come from
\pkg{FSelector} package \citep{FSelector}, but we provide many technical
improvements. Our reimplementation occures to have shorter computation
times, it does not require earlier Java nor Weka
\citep{Hall:2009:WDM:1656274.1656278} installation and provides support
for sparse matrix format of data, e.g.~presented in \pkg{Matrix} package
\citep{Matrix}. This approach facilitates software installation and
improves work with bigger datasets, in comparison to the base
\R ~implementation in \pkg{FSelector}, which is even not optimal in the
sense of \R ~code. \par Additionally, we present new, C++ implementation
of the discretization method for continuous variables called
Multi-Interval Discretization (MDL) method \citep{Fayyad1993} that is
based on \emph{Minimum Description Length} critetion
\citep{rissanen1986} and is required in entropy calculations during the
feature selection process in showed methods. By default, regular
\pkg{FSelector} implementation uses \pkg{RWeka} package
\citep{Hornik2009} for discretization and \pkg{entropy} \citep{entropy}
for entropy calculations - for both we also attach the computation times
comparison. \par Finally, we announce the full list of available
functions, which are divided to 2 groups: entropy based feature
selection methods and stepwise attribute selection functions that might
use any evaluator to choose propoer features, e.g.~presented entropy
based algorithms.
}

\section{Introduction and Motivation}\label{introduction-and-motivation}

In modern statistical learning the biggest bottlenecks are computation
times of model training procedures and the overfitting. Both are caused
by the same issue - the high dimension of explanatory variables space.
Researchers have encountered problems with too big sets of features used
in machine learning algorithms also in terms of model interpretaion.
This motivates applying feature selection algorithms before performing
statisical modeling, so that on smaller set of attributes the training
time will be shorter, the interpretation might be clearer and the noise
from non important features can be avoided. More motivation can be found
in \cite{John94irrelevantfeatures}. \par Many methods were developed to
reduce the curse of dimensionality like Principal Component Analysis
\citep{PCA:14786440109462720} or Singular Value Decomposition
\citep{eckart1936approximation} which approximates the variables by
smaller number of combinations of original variables, but this approach
is hard to interpret in the final model. \par Sophisticated methods of
attribute selection as Boruta algoritm \citep{Boruta}, genetic
algorithms \citep{geneticAlgo, FedCSIS2013l106} or simulated annealing
techniques \citep{Khachaturyan:a19748} are known and broadly used but in
some cases for those algorithms computations can take even days, not to
mention that datasets are growing every hour. \par Few classification
and regression models can reduce redundand variables during the training
phase of statistical learning process, e.g.~Decision Trees
\citep{Rokach:2008:DMD:1796114, cart84}, LASSO Regularized Generalized
Linear Models (with cross-validation) \citep{glmnet} or Regularized
Support Vector Machine \citep{Xu:2009:RRS:1577069.1755834}, but still
computations starting with full set of explanatory variables are time
consuming and the understaning of the feature selection procedure in
this case is not simple and those methods are sometimes used without the
understanding. \par In business applications there appear a need to
provide a fast feature selection that is extremely easy to understand.
For such demands easy methods are prefered. This motivates using simple
techniques like Entropy Based Feature Selection
\citep{Largeron:2011:EBF:1982185.1982389}, where every feature can be
checked independently so that computations can be performed in a
parallel to shorter the procedure's time. For this approach we provide
an \R ~interface to Rcpp reimplementation \citep{Rcpp} of methods
included in \pkg{FSelector} package which we also extended with parallel
background and sparse matrix support. This has significant impact on
computations time and can be used on greater datasets, comparing to
\pkg{FSelector}. Additionally we avoided the Weka
\citep{Hall:2009:WDM:1656274.1656278} dependency and we provided faster
discretization implementations than those from \pkg{entropy} package,
used originally in \pkg{FSelector}.

\section{Discretization}\label{discretization}

In statistical modelling, the \textbf{discretization} is the process of
transferring continuous explanatory variables into discrete
counterparts. Problems caused by categorization of continuous variables
are known and widely spread \citep{HarrellDISC}, but in some cases there
appear an algorithmic requirement for the discretization. Moreover,
there exist few algorithms, like decision trees \citep{Salzberg1994},
where continuous attributes are discretized during the learning process.
Other reason for variable discretization include increasing the speed of
induction algorithms \citep{Catlett1991}. \par Even though many
categorization algorithms have been developed
\citep{Holte1993, Chan169942}, in this chapter we focus on a recursive
entropy minimization heuristic for categorization coupled with a
\emph{Minimum Description Length} critetion \citep{rissanen1986} that
controls the number of intervals produced over the continuous space.
This is motivated by the original usage of this algorithm in
\pkg{FSelector} package, which is our baseline. As
\cite{Dougherty95supervisedand} showed better performance of
classification for discretized feature set on real-world datasets and
states that the described method was found promising by the authors not
only for the local discretization but also for global discretization
\citep{Ting94discretizationof}.

\subsection{Entropy Based Minimum Description Length Discretization
Method}\label{entropy-based-minimum-description-length-discretization-method}

In this section we pressent the overview of the method, but before that
let us introduce some terminology, after \citep{Fayyad1993}.

\par 

Assume we are to discretize continuous variable \(A\), having set \(S\)
of \(N\) observations. As a \emph{cut point}, we call such a treshold
value \(T\) that \(A<=T\) assigns observations to left interval of S and
\(A>T\) assigns observations to right interval of
S\footnotetext{The $A > T$ means: the values of $A$ are greater than $T$}.
For the ordered \(A\) the algorithm is looking for the best cut point
\(T_A\) from the range of \(A\) by evaluating \emph{every candidate cut
point} in the range of values. For each evaluation of a candidate cut
point \(T\), the data are partitioned into two sets and the class
entropy of resulting partition is calucalted.

\par 

Let \emph{T} partition the set \(S\) into subsets \(S_1\) and \(S_2\).
For \(k\) classes (\(C_1,\dots,C_k\)) let \(\mathbb{P}(C_i, S)\) be
proportion of observations that belongs to class \(C_i\). The
\emph{class entropy} of a subset S is defined as:
\[Ent(S) = - \sum_{i=1}^{k}\mathbb{P}(C_i, S)\log_{2}\mathbb{P}(C_i, S).\]

This is said to to measure the information needed (in \emph{bits}) to
specify the classes in \(S\). Having this definition one can specify,
for partitioned set \(S\), the weighted average of the resulting class
entropies:

\textbf{Definition 1.} For an example set \(S\), an attribute \(A\) and
a cut value \(T\); let \(S_1 \subset S\) be the subset of observations
in \(S\) with \(A\) values \(<= T\) and \(S_2 = S - S_1\). The
\emph{class information entropy of the partition induced by T, E(A, T;
S)}, is defined

\begin{equation}
E(A, T; S) = \frac{|S_1|}{S}Ent(S_1) + \frac{|S_2|}{S}Ent(S_2).
\end{equation}

A binary discretization for \(A\) is determined by selecting the cut
point \(T_A\) for which \(E(A, T; S)\) is minimal amongst all the
candidate cut points.

\par 

Choosing beetwen all possible values of \(A\), when looking for optimal
\(T_A\), could be very time and computations consuming, but in
\cite{Fayyad:1992:IDT:144532} it is proved that if \(T\) is an optimal
cut point (minimizes the E(A, T; S)) then \(T\) is a \emph{boundary
point}.

\textbf{Definition 2.} A value \(T\) in the range of \(A\) is a
\emph{boundary point} if in the sequence of examples sorted by the value
of \(A\), there exist two points \(\epsilon_1, \epsilon_2 \in S\),
having different classes, such that
\(A(\epsilon_1) < T < A(\epsilon_2)\); such that there exists no other
example \(\epsilon^1 \in S\) such that
\(A(\epsilon_1) < A(\epsilon^1) < A(\epsilon_2)\), where \(A(\epsilon)\)
denote the \(A\) value of example \(\epsilon \in S\).

\begin{itemize}
\tightlist
\item
  Backward compatibility with \pkg{FSelector}
\end{itemize}

\begin{Schunk}
\begin{Sinput}
library(RWeka)
library(FSelectorRcpp)
RWeka::Discretize(Species~Sepal.Length, data = iris)[, 1] -> Rweka_disc_out
FSelectorRcpp::discretize(iris$Sepal.Length, iris$Species) ->FSelectorRcpp_disc_out
table(Rweka_disc_out,FSelectorRcpp_disc_out)
\end{Sinput}
\begin{Soutput}
               FSelectorRcpp_disc_out
Rweka_disc_out  (-Inf;5.550000] (5.550000;6.150000] (6.150000;Inf)
  '(-inf-5.55]'              59                   0              0
  '(5.55-6.15]'               0                  36              0
  '(6.15-inf)'                0                   0             55
\end{Soutput}
\end{Schunk}

\begin{itemize}
\tightlist
\item
  Time comparison on small datasets
\end{itemize}

\begin{Schunk}
\begin{Sinput}
library(microbenchmark)
microbenchmark(discretize(iris$Sepal.Length, iris$Species),
               Discretize(Species~Sepal.Length, data = iris))
\end{Sinput}
\begin{Soutput}
Unit: microseconds
                                            expr      min       lq      mean    median       uq
     discretize(iris$Sepal.Length, iris$Species)  143.331  247.405  266.4419  278.5305  297.222
 Discretize(Species ~ Sepal.Length, data = iris) 5147.027 5670.589 6554.0072 6036.0655 7145.927
       max neval cld
   351.111   100  a 
 11871.580   100   b
\end{Soutput}
\end{Schunk}

\begin{itemize}
\item
  Comparison on big datasets
\item
  Plot comparison
\end{itemize}

\section{Entropy Based Feature Selection
Algorithms}\label{entropy-based-feature-selection-algorithms}

In the information theory the term \textbf{entropy}
\citep{Shannon:2001:MTC:584091.584093} is

\section{Stepwise Attribute Selection
Evaluators}\label{stepwise-attribute-selection-evaluators}

\section{FSelectorRcpp and FSelector Computation Times
Comparison}\label{fselectorrcpp-and-fselector-computation-times-comparison}

\section{Conclusion}\label{conclusion}

\section{Acknowledgment}\label{acknowledgment}

\bibliography{RJreferences}

\address{%
Zygmunt Zawadzki\\
\\
\\
}
\href{mailto:zygmunt@zstat.pl}{\nolinkurl{zygmunt@zstat.pl}}

\address{%
Marcin Kosi≈Ñski\\
Warsaw Univeristy of Technology\\
Faculty of Mathematics and Information Science\\ Koszykowa 75, Warsaw Poland\\
}
\href{mailto:m.kosinski@mini.pw.edu.pl}{\nolinkurl{m.kosinski@mini.pw.edu.pl}}

